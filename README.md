# Data Collection â€“ Web Scraping

This repository contains data collected through web scraping.  
It includes raw HTML files fetched from websites and CSV files generated after extracting useful information using web scraping techniques.

The project demonstrates the complete data collection workflow using **Requests** for fetching web pages and **BeautifulSoup** for parsing and extracting data.

---

## ğŸ“ Repository Contents

- **HTML Files**
  - Raw web pages saved locally after scraping
  - Used as source data for extraction

- **CSV Files**
  - Structured data extracted from HTML files
  - Ready for analysis or further processing

- **Scripts / Notebooks**
  - Code used to collect web data
  - Parsing and extraction logic using BeautifulSoup

---

## ğŸ”§ Tools & Libraries Used

- Requests â€“ for sending HTTP requests
- BeautifulSoup (bs4) â€“ for parsing HTML
- lxml â€“ fast HTML parser
- CSV â€“ for storing structured data

---

## ğŸš€ Workflow

1. Fetch web pages using HTTP requests  
2. Save raw responses as HTML files  
3. Parse HTML using BeautifulSoup  
4. Extract required information  
5. Store extracted data in CSV format  

---

## â–¶ï¸ How to Use

1. Explore the HTML files to understand page structure  
2. Run the scraping scripts or notebooks  
3. Generate CSV files from extracted data  
4. Use CSV files for analysis or learning purposes  

---

## ğŸ“Œ Notes

- This repository is created for **educational and learning purposes**
- Avoid excessive requests to servers

---

## ğŸ‘©â€ğŸ’» Author

**Shivangi**
